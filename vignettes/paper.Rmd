---
title: "eplusr: A framework for integrating building energy simulation and data-driven analytics"
author:
  - name: Hongyuan Jia
    email: hongyuan.jia@bears-berkeley.sg
    affiliation: SinBerBEST
  - name: Adrian Chong
    email: adrian.chong@nus.edu.sg
    affiliation: NUS
    footnote: 1
address:
  - code: SinBerBEST
    address: |
      SinBerBEST Program, Berkeley Education Alliance for Research in Singapore,
      Singapore, 138602, Singapore
  - code: NUS
    address: |
      Department of Building, School of Design and Environment, National
      University of Singapore, 4 Architecture Drive, Singapore, 117566,
      Singapore
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  Building energy simulation (BES) has been widely adopted for the
  investigation of environmental and energy performance for different design
  and retrofit alternatives. In BES, data-driven analytics is of commensurate
  importance in order to turn BES results into understanding, insight, and
  knowledge. However, there is no widely adopted solution to provide seamless
  integration of BES and data-driven analytics. This paper presents a newly
  developed framework 'eplusr' for conducting parametric analysis using
  EnergyPlus via the R programming language. With a data-centric design
  philosophy, the proposed framework focuses on better and more seamless
  integration between BES and data-driven analytics. It provides a Tidy data
  format for BES that matches the semantics of the simulation results which can
  be easily fed to various data analytics workflows in R. The framework also
  provides an infrastructure to bring portable and reusable computation
  environment for building energy modeling using, which aims to facilitate the
  reproducibility research in building energy domain. This paper discusses the
  philosophy behind the framework, its architecture and core capabilities, and
  demonstrates the streamlined workflow of using this framework to conduct data
  exploration, parametric simulation, multi-objective optimization, and
  Bayesian calibration.
journal: "Automation in Construction"
date: "`r Sys.Date()`"
bibliography: references.bib
layout: 3p, times
colorlinks: yes
link-citations: yes
linenumbers: false
output:
  bookdown::pdf_book:
    includes:
      in_header: header.tex
    base_format: rticles::elsevier_article
    keep_tex: yes
---

```{r setup, include = FALSE}
library(eplusr)
library(kableExtra)
library(here)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  eval = FALSE,
  comment = "#>",
  out.width = "\\columnwidth",
  fig.path = "../figures/",
  fig.pos="ht"
)

# code chunk cross-ref
Chunk <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function (x, options) {
    x <- Chunk(x, options)
    if (is.null(options$label)) return(x)
    if (!options$echo) return(x)
    if (!is.null(options$fig.cap)) return(x)
    # use chunk label as cross-ref label
    x <- paste0(
        # use Pandoc raw-attribute to preserve raw LaTeX content
        # https://pandoc.org/MANUAL.html#extension-raw_attribute
        "`\\begin{Shaded}`{=latex}\n",
        x, "\n",
        "\\captionof{code}{", options$code.cap, "\\label{code:", options$label, "}}\n",
        "`\\end{Shaded}`{=latex}\n"
    )
    x
})
```

# Highlights {.unnumbered}

1. A framework integrates EnergyPlus with data science through R
2. The framework includes a parametric simulation  with flexibility and extensibility
3. Tidy data format is introduced to facilitate data-driven analytics of BES research
4. Docker together with R project management can be adopted to bring reproducible BESs

# Introduction

Building energy simulation (BES) is increasingly being used for the analysis and
prediction of building energy consumption, measurement and verification,
life-cycle carbon evaluation and cost analysis of energy conservation measures
(ECMs) [@Chong2017; @Kneifel2010]. It has played a growing role in the design
and operation of low energy, high-performance buildings and development of
policies that drive the achievement of reducing energy use and greenhouse-gas
emissions in the buildings sector [@Hong2018].

BES offers an alternative approach that encourages customized, integrated design
solutions and the development of BES tools has been pronounced over the decades
[@Hong2000; @Hong2018]. The core tools in the BES field are the whole-building
energy simulation programs which provide users with key building performance
indicators such as energy use and demand, temperature, humidity, and costs
[@Crawley2008a]. Among them, EnergyPlus is one of the widely used whole
building energy simulation tools [@Crawley2001] and has been adopted as a core
component or a building simulation engine of energy performance assessment in
both free and commercial building design and analysis tools [@See2011; @DesignBuilderSoftwareLtd2020; @Guglielmetti2011; @Yi2020; @Roudsari2013].

Parametric analysis in BES has been a powerful approach to enable investigation
of environmental and energy performance for different design and retrofit
alternatives. Choosing the appropriate combination of design options is
a complex task that requires the management of a large amount of information
on the properties of design options and the simulation of their performance
[@Purup2020]. Parametric analysis involves tedious file management tasks,
repeated entry of model parameters, the application of design transformations
and the execution of large-scale analyses [@Macumber2012], which can be
time-consuming and error-prone. Parametric simulation task automation has been approved to
be an extremely useful way to reduce human intervention and improve the
efficiency of large parametric analysis. Multiple efforts have been made with
this regard and there are several existing free EnergyPlus-based parametric
simulation frameworks developed, as shown in Table \@ref(tab:pat-sum). Some frameworks
consist of graphical user interfaces (GUIs) built on EnergyPlus while some use
general-purpose scripting languages with a full complement of programming
features and libraries [@Roth2018].

\setlength{\tabcolsep}{0.1pt}
\renewcommand{\arraystretch}{1.3}
```{r pat-sum, eval = TRUE, include = TRUE, echo = FALSE}
# TODO: Add references for mentioned frameworks after changing the bibliography
#       style to numeric
compare <- data.table::fread(here::here("data-raw/compare.csv"))

# use tick symbol
compare[, c(names(compare)) := lapply(.SD, function (val) {val[val == "X"] <- "\\cmark";val})]

# columns that should be further described in footnotes
cols <- c("Semantic API", "Support optimization", "Data extraction", "Weather data handling", "Post-processing capabilities")
data.table::setnames(compare, cols, paste0(cols, kableExtra::footnote_marker_number(seq_along(cols), "latex")))

compare %>%
    .[Name == "Ladybug & Honeybee",  Name := "Ladybug \\& Honeybee"] %>%
    knitr::kable(booktabs = TRUE, format = "latex", linesep = "", escape = FALSE,
        align = c("l", "c", "c", "c", "c", "c", "c", "c", "c", "c", "c", "c"),
        caption = "Comparison of parametric simulation frameworks based on EnergyPlus") %>%
    kableExtra::kable_styling(font_size = 7, latex_options = "hold_position") %>%
    # have to manually specify the column width since 'full_width' cannot be used here
    # ref: https://github.com/haozhu233/kableExtra/issues/39
    # name
    kableExtra::column_spec(1, "2.8cm") %>%
    # cross-platform
    kableExtra::column_spec(2, "1.2cm") %>%
    # GUI
    kableExtra::column_spec(3, "0.5cm") %>%
    # Open-source
    kableExtra::column_spec(4, "1.1cm") %>%
    # Semantic API
    kableExtra::column_spec(5, "1.1cm") %>%
    # API language
    kableExtra::column_spec(6, "1.3cm") %>%
    # Support optimization
    kableExtra::column_spec(7, "1.6cm") %>%
    # Support calibration
    kableExtra::column_spec(8, "1.6cm") %>%
    # Data extraction
    kableExtra::column_spec(9, "1.4cm") %>%
    # SQL-based structural output
    kableExtra::column_spec(10, "1.5cm") %>%
    # Weather data handling
    kableExtra::column_spec(11, "1.3cm") %>%
    # Post-process capabilities
    kableExtra::column_spec(12, "1.6cm") %>%
    kableExtra::add_footnote(threeparttable = TRUE, notation = "number", c(
        # Semantic API
        "Further abstraction classes to directly perform geometry transformations, HVAC system manipulation and etc.",
        # Support optimization
        "Only built-in features are considered. So as for calibration support. Some frameworks can be further coupled with other software or libraries to perform optimizations",
        # Data extraction
        "The capabilities of extracting further customized summary data, instead of solely based on EnergyPlus built-in functionalities",
        # Weather data handling
        "The capabilities of extracting and modifying data from weather files",
        # Post-processing capabilities
        "The capabilities of performing further data analyses on the extracted simulation results. 'H', 'M' and 'L' indicates 'High', 'Medium' and 'Low', respectively"
    )) %>%
    # change footnote font size manually
    gsub("\\small", "\\scriptsize", x = ., fixed = TRUE)
```

OpenStudio [@Guglielmetti2011] is a free open-source software toolkit designed
for energy modeling and can be used to efficiently create or modify models,
manage individual or multiple simulations, and visualize results. OpenStudio
has its own format (`.OSM`) and schema for EnergyPlus model representation
which will eventually be translated into EnergyPlus models. Parametric Analysis
Tools (PAT) is a GUI application that is part of OpenStudio toolkit. It aims to
enable customizable and shareable parametric descriptions of ECMs
[@Parker2014]. It leverages OpenStudio Measures which are reusable scripts
written in Ruby programming language to manipulate OpenStudio models and can be used to compare
manually specified combinations of measures, optimize designs, calibrate models
and perform parametric sensitivity analysis. Ladybug and Honeybee are plug-ins
developed for Rhino Grasshopper [@Roudsari2013]. Ladybug is capable to import
and analyze EnergyPlus standard weather data (EPW) in Grasshopper, while
Honeybee can create, run and visualize the OpenStudio and EnergyPlus simulation
results. Both Ladybug and Honeybee take full advantage of the visual scripting
interfaces provided by Grasshopper [@Tabadkani2019]. Since the primary input
files of EnergyPlus are all ASCII text-based, it is possible to update building
energy models by directly processing and manipulating the text files, without taking
into account the complex hierarchical structure in the model components. There
are several applications that take this approach. jEplus [@Yi2020] is a
software written in Java programming language to perform complex parametric analysis on multiple
design parameters. It allows users to describe the parameters and their values
using customized symbols in a graphical user interface, and automatically create
parametric models using text-substitution [@Zhang2010a]. Modelkit
[@BigLadderSoftware2020] is another free and open-source framework for parametric
model using EnergyPlus. Compared to jEplus, it is capable of automating the
generation and management of EnergyPlus models via its templates and scripting
tools written in Ruby. MLE+ integrates EnergyPlus and scientific computation
and design capacities of Matlab for controller design and can be used to
implement and simulate advanced control algorithms of building systems
[@Bernal2012; @Zhao2013b]. eppy [@Philip2020] is a library for manipulating
EnergyPlus models programmatically via python programming language. It parses
EnergyPlus IDF files into a python object and provides low-level programmatic
access to EnergyPlus inputs. EpXL [@Schild2020] is an EnergyPlus
Microsoft Excel user-interface written in Visual Basic for Applications (VBA)
that enables import and export of IDF data files, parametric analysis and
optimization. It is capable of displaying a compact tabular overview of input
data and automatically importing simulation output into Excel, with a link for
viewing the 3D model. GenOpt is a generic optimization programme that can be
used with EnergyPlus to minimize an objective function with respect to multiple
parameters [@Wetter2001]. It generates new input templates files by replacing
the keywords with corresponding numerical values calculated using a
mathematical optimization library. The frameworks mentioned above may have
overlapping in features but they are tailored for different purposes and use
cases, with the primary focus on ease the time-consuming and error-prone
process of creating and managing parametric simulations.

Data-driven analytics are complementary with BES in nature [@Srivastava2019]
aiming to build systems and algorithms to discover knowledge, detect patterns,
and generate useful insights and predictions from BES data [@Molina-Solana2017;
@BurakGunay2019]. It encompasses the whole data science process, beginning from
data extraction and cleaning, and extending to data analysis, description and
summarization. BES, with an iterative nature inside, can produce a large amount
of data. The volumes of BES data have clearly overwhelmed traditional data
analysis methods such as spreadsheets and ad-hoc queries with large amount of
factors to be considered [@Kim2011]. Moreover, the output of common BES tools
is not always friendly in format for applying these methods which makes data
pre-processing an essential but time-consuming and laborious process for any
data-driven analytics for BES data. This highlights the potential areas for
improvements in data extraction and result presentation in a clear and
intuitive manner for data analytics. Even some BES tools provide summary
reports with various details, it is still quite common in BES to perform
post-processing and apply customized and more advanced algorithms to the
simulation results. Currently, there is a growing body of scientific literature
on the application of advanced mathematical algorithms for building design
using BES [@Kiss2020]. Researches on parametric building energy simulations
that employ data mining algorithms upon large amounts of simulations have
emerged [@BurakGunay2019]. Solutions in most existing frameworks have limited
post-processing capacities and are not flexible enough to enable a clear
understanding and control on how the BES data is being transformed
[@Miller2013; @Attia2013a]. Open-source programming environments such as R
[@RCoreTeam2019] and Python [@Oliphant2007] are promising on providing
solutions for large-scale data analytics and have become widely-used research
tools that provide access to many well-documented packages for various data
mining, machine learning and data visualization applications [@Lowndes2017;
@Molina-Solana2017]. However, fewer efforts have been made in terms of
providing a seamless integrated approach to bridge the gap between BES and
data-driven analytics. Results of a survey of 448 building energy
management professionals in the U.S. show that there is a need of improving the
efficacy and integration of data-driven analytics and BES in the building
energy management domain, and efforts should be made to develop integrated
tools that are capable of leveraging both methods [@Srivastava2019].

BES is a complex domain that involves multiple scientific processes and is
often a time-intensive, error-prone and hard-to-reproducible process.
Reproducibility is defined as the ability to recompute data analytic results,
given an observed data set and knowledge of the data analysis pipeline
[@Peng2015]. Reproducible research has received an increasing level of
attention throughout the scientific community and the public at large
[@Boettiger2015]. According to a Nature's survey of 1576 researchers in 2016,
more than 70% of them failed to reproduce another scientist's experiments, and
more than 50% failed to reproduce their own experiments. Moreover, more than
half of them agreed that there was a significant crisis of reproducibility
[@Baker2016a]. As BES becomes more and more integral to many aspects of architecture design,
and decision-making processes, computational reproducibility has become an
issue of increasing importance to researchers, designers and practitioners.
Lack of credibility in BES results due to a lack quality and reproducibility is
widely considered a problem by the energy model community [@Fleming2012].
Despite the culture of reluctance and a lack of requirements or incentives to
publish the code used in generating the results, the reasons why BES is usually
not easy to reproduce is mainly caused by 2 aspects:

1. *A lack of seamless integration of BES and data-driven analytics workflows*

   When conducting BES, most users prefer to use GUI applications which makes it
   intuitive and easy to execute certain particular tasks. However, GUI tools have
   constraints on flexibility as the users have to specify exactly what and how
   features of the design can be manipulated and often are not be able to provide
   a good workflow for repeating that task across a wider range of situations on
   different systems. In this case, manual steps have to be performed using other
   tools, such as a spreadsheet or command-line tools, which introduces
   additional transcription burden and results in a non-reproducible process
   [@Macumber2012]. Sometimes, custom solutions have to be created from scratch to
   automate part portions of the workflows, which may lead to new inefficiencies
   and potential errors.

   \vspace{5pt}

   One of the goals of BES is to turn raw simulation data into understanding,
   insight, and knowledge. BES projects often encompass an iterative workflow of
   raw design data importing and tidying, parametric input file generation and
   modification, simulation result collection, processing and analyses, and
   finally, conclusion visualization. Most of the time, data analytics of BES
   results is of commensurate importance with the model itself. Currently,
   there is no widely adopted solution that is able to integrate all processes
   into one single platform.

2. *Hard to setup the computational environment which covers the whole BES and data-driven analytics toolchain*

   BES often involves the use of multiple applications, software and platforms.
   To perform crucial scientific processes such as
   replicating the results, extending the approach or testing the conclusions in
   other contexts, the indispensable step is to install the software used by the
   original researchers, which sometimes can become immensely time-consuming if
   not impossible. It is easy to underestimate the significant barriers raised by
   a lack of familiar, intuitive, and widely adopted tools for addressing the
   challenges of computational reproducibility [@Boettiger2015].

In summary, multiple efforts have been made to improve the wide adoption of BES
among researchers, designers and practitioners. However, an overall holistic
framework to bridge the gap between the building energy simulation and data
science domains is still undefined. There is also a need for an integrated
framework to provide a supportive environment to bring reproducible building
energy modeling.

In this paper, we introduce a new framework called 'eplusr' for integrating BES
and data-driven analytics. Eplusr is different from existing frameworks because
of its data-centric design philosophy. It provides a parametric simulation
prototype with a unified data interface ensuring BES data is returned in a
consistent format to match the semantics of the simulation results. This makes
the BES data can be seamlessly integrated into data-driven analytics workflows
with minor processing. The proposed framework also provides infrastructures to
bring portable and reusable computation environment for BES, aiming to
facilitate the reproducibility research in building energy domain. This paper
first introduces the concepts and behind the framework and its modules, along
with its implementation. Next, the applications of the framework using a medium
office building model is presented, covering various topics including data
exploration, parametric simulation, optimization and calibration.

# Methodology

The main goals of this framework is to:

1. provide better and more seamless integration between BES and data-driven
   analytics tools. A unified data interface is developed which makes sure all
   simulation data are always stored in a consistent form that matches the
   semantics of the simulation results and thus can be easily fed to various data
   mining and machine learning algorithms using existing tools in R.
2. provide a rich-featured library that contains a set of abstractions to help
   modify EnergyPlus simulation inputs in a programmatic way and provides a
   simple yet flexible and extensible parametric simulation prototype which can
   be easily extended to perform sensitivity analysis, model calibration and
   optimization.
3. provide infrastructure to bring portable and reusable computation
   environment for building energy model using, which aims to facilitate
   the reproducibility research in building energy domain.

Fig. \@ref(fig:architecture) shows an overview of the eplusr framework. Eplusr contains 2 main parts:

1. An R package called eplusr which integrates whole building simulation
   software EnergyPlus and the data-driven analytics environment powered by the
   R programming language.
2. A computation environment powered by Docker that contains all necessary
   tools to facilitate reproducible BES.

```{r architecture, echo = FALSE, eval = TRUE, fig.cap = "Eplusr framework architecture", out.width = "40%"}
knitr::include_graphics(here("figures/overview.png"))
```

## The Structure of eplusr package

The eplsur R package is the core component of the framework which provides a
programmatic interface of modifying EnergyPlus models, data interface to
extract simulation results. The R package consist of 5 modules:

1. I/O Processing & Parser
2. Model API (Application Program Interface)
3. Tidy Data Extractor
4. Simulation Manager
5. Parametric Prototype

### Parser and Model API

In eplusr, the I/O processing & parser model aims to read and parse EnergyPlus
models and store the data in a format that is both flexible to modify and
capable to store the hierarchical relationship of various model components.
EnergyPlus uses a text input file format named IDF (Input Data File) as a model
file, and each version of an IDF file has a corresponding version of an IDD
(Input Data Dictionary) file who works as a data format schema. In eplusr,
`Idd` class and `IddObject` class provides parsing and data-storing
functionality of an IDD file. These data will be used for subsequent parsing
and data verification of all IDFs of that version. This makes eplusr be able to
handle various versions of IDF files at the same time easily.

Under the hook, eplusr uses a SQL-like structure to store both IDF and IDD data
in various tables, as shown in Fig. \@ref(fig:data-structure). Data of an IDF
is stored in three tables, i.e. object, value, and reference while data of an
IDD is stored in four tables, i.e. group, class, field and reference. With this
data structure, to modify an EnergyPlus model in eplusr is equal to change the
data in those `Idf` tables accordingly, in the context of specific `Idd` data.
In this sense, all methods in `Idf` and `Idd` classes are just interface
wrappers which expose the modification process in a more user-friendly manner.
All the data are masked from end-users but can be easily extracted when needed.

```{r data-structure, echo = FALSE, eval = TRUE, fig.cap = "Data structure of an Idf and Idd object", out.width = "70%"}
knitr::include_graphics(here("figures/data_structure.png"))
```

For most of the time, users do not need to directly interact with `Idd` and
`IddObject` objects, but instead simply using `Idf` and `IdfObject` objects to
modify an IDF file, which provides flexible yet powerful model modification APIs.
Both `Idf` and `IdfObject` classes contain around thirty member functions for
low-level programmatic access to and modification of the IDF data. Every
modification on the IDF data will be verified to make sure the result complies
with underlying IDD.

`Idf` class provides rich-featured interfaces to modify models via various
scope, including scope of a single object, scope of grouped objects level, and
scope of all objects in a certain class.

```{r idf-set, code.cap = "Example of model API", echo = TRUE}
model$set(
  `Supply Fan 1` = list(Fan_Total_Efficiency = 0.9),

  .("Metal Decking", "Metal Roofing") :=
      list(Roughness = "Smooth", Thickness = 0.003),

  Lights := list(Watts_per_Zone_Floor_Area = 7.0)
)
```

Listing \ref{code:idf-set} demonstrates the flexible value modifying scopes
provided by `Idf$set()` method. `model` here is an `Idf` object. In Line 2, the
total efficiency of a `Fan:VariableVolume` object named `Supply Fan 1` will be
changed to 0.9; In line 4-5, the roughness and thickness of two materials named
`Metal Decking` and `Metal Roofing` will be changed; While in line 7, the
lighting power density of all lights will be changed to 7.0 $\mathrm{W}/\mathrm{m}^2$.

Besides of `Idf$set()`, `Idf$update()` method provides an interface that can
achieve the same results but with a `data.frame` input. `data.frame` is an R
representation of a table where each column contains values of one variable. In
Listing 3, Line 1, 4 and 8 extract object data into a `data.table`s
[@Dowle2019], an extension of `data.frame` extremely optimized for speed.
Line 2, 5, 6 and 9 are used to modify certain field values. Line 11-13 are then
call `Idf$update()` method to update object data.

Users can perform any modifications to that data and then update corresponding
object values by feeding the updated data to `Idf$update()` method.

The weather data format used in EnergyPlus is called EPW (EnergyPlus Weather),
a simple, text-based format with comma-separated data. eplusr introduces the
`Epw` class to directly extract both metadata and core weather data of an EPW
file. Moreover, `Epw` class is capable of creating new and modifying existing
EPW files, which is usually an essential process when doing model parameter
calibration.

### Tidy data extractor

When extracting simulation results, instead of directly reading and
manipulating the CSV and HTML files, eplusr uses EnergyPlus SQL output to
extract results of `Output:Variable`, `Output:Meter*`, and `Output:Table`
objects specified in the IDF. The main benefit of this approach is that it
makes sure eplusr always returns the simulation results in a *Tidy Data* format.

Tidy data format was first proposed by @Wickham2014, which is a standard way of
mapping the meaning of a dataset to its structure. A dataset is messy or tidy
depending on how rows, columns and tables are matched up with observations,
variables and types. In tidy data:

* Each variable forms a column.
* Each observation forms a row.
* Each type of observational unit forms a table.

Tidy data makes it easy for an analyst or a computer to extract needed
variables because it provides a standard way of structuring a dataset. Tidy
data is particularly well suited for vectorized programming languages like R,
because the layout ensures that values of different variables from the same
observation are always paired.

However, EnergyPlus CSV output does not present data in a tidy way. Taking
Table \@ref(tab:mess-csv) an example. We are facing one main problem when
working with this type of EnergyPlus CSV output, i.e. column headers contain
values, not only variable names.

In EnergyPlus CSV output, column headers are composed in format *Key value*
(`MEETING_ROOM_1`) + *Variable name* (`Zone Total Internal Gain Rate`) +
*Units* (`W`) + *Reporting frequency* (`Hourly`). This format often leads to
additional data cleaning efforts. For instance, to get all outputs for variable
`Zone Total Internal Latent Gain Rate`, users have to write their methods of
splitting column headers into different parts or subsetting column using
regular expressions.

When using eplusr, the same results in Table \@ref(tab:mess-csv) are always
presented in a tidy format, as shown in Table \@ref(tab:tidy-csv). It
transforms the original column headers from EnergyPlus CSV outputs into four
separate columns, i.e. `key_value`, `name`, `units` and `reporting_frequency`.
Despite those changes, eplusr will also add an additional column named `case`,
which is by default the IDF file name without extension. `case` column can be
quite useful and serve as an indicator to separate each simulation results when
extracting outputs from multiple parametric simulations.

In summary, giving data in a tidy format has several advantages in R, as it
will make sure the data is always fit for directly handling by the *tidyverse*
[@Wickham2019a] package ecosystem, which is a language for solving data-driven analytics
challenges with R code.

```{r mess-csv, echo = FALSE, eval = TRUE}
knitr::kable(data.table::fread(here("data-raw/mess.csv")), linesep = "",
    caption = "Example of EnergyPlus CSV output", format = "latex", booktabs = TRUE) %>%
    kable_styling(font_size = 8) %>%
    column_spec(1, "2.5cm")
```

\setlength{\tabcolsep}{1.0pt}
```{r tidy-csv, echo = FALSE, eval = TRUE}
knitr::kable(data.table::fread(here("data-raw/tidy.csv")), linesep = "", align = "c",
    caption = "Tidy format of EnergyPlus output", format = "latex", booktabs = TRUE) %>%
    kable_styling(font_size = 8) %>%
    column_spec(1, "0.9cm") %>%
    column_spec(2, "2.6cm") %>%
    column_spec(3, "1.5cm") %>%
    column_spec(4, "5.2cm") %>%
    column_spec(5, "0.7cm") %>%
    column_spec(6, "2.9cm")
```

Fig. \@ref(fig:data-interface) shows the schematic diagram of eplusr tidy data
interface. Unlike the CSV format output, EnergyPlus's SQL output stores all
simulation results in various tables. For instance, Table. \@ref(tab:tidy-csv)
will be a result of joint of tables `Time`, `EnvironmentPeriods`,
`ReportDataDictionary` and `ReportData`, where the `Time` table contains the
time of simulation, including month, day, hour, day of the week and etc, the
`EnvironmentPeriods` table contains the names and types of the simulated run
period, the `ReportDataDictionary` table provides the dictionary of reported
variables including their parent model component names, variable names, units
and reported frequency, and the `ReportData` contains the actual values of
reported variables. Eplusr result extraction interface provides the
possibilities to query any of the mentioned information which makes it easy and
quite straightforward to get simulation results of any specified time, units
and variables in a consistent manner.
Moreover, instead of presenting the simulated date and time as strings shown in
Table \@ref(tab:mess-csv), eplusr will calculate a proper year value for each
run period and compose a series of `DateTimeClasses` values in the `datetime`
column. This makes it quite straightforward to apply further time-series
analysis on the simulation results.

```{r data-interface, echo = FALSE, eval = TRUE, fig.cap = "Tidy data interface", out.width = "80%"}
knitr::include_graphics(here("figures/result_extraction_interface.png"))
```

### The parametric prototype

eplusr provides a class `ParametricJob` to do parametric simulations. It is
simple to use but is also quite flexible and extensible. It takes full
advantages of eplusr model editing and result collecting functionalities.
Thus, the `ParametricJob` class is capable of defining various analyses using
any algorithms available in R, including but not limited to sensitivity
analysis, optimizations, and model calibration. The overall process of the
framework is shown in Fig. \@ref(fig:parametric) and can be summarized as
follows:

```{r parametric, echo = FALSE, eval = TRUE, fig.cap = "Workflow of a parametric simulation", out.width = "70%", fig.pos = "H"}
knitr::include_graphics(here("figures/parametric.png"))
```

1. Initialize a `ParametricJob` object. Creating a parametric job in eplusr is
   just a simple call of the constructor `param_job()` function. It takes an
   IDF file as the *seed* and an EPW file as the *weather* to be used, as shown
   in Listing 6.

2. Construct a measure function. Here, the concept of measure in eplusr is
   inspired by "measures" in OpenStudio [@Guglielmetti2011]. Basically, a
   measure is a function that takes an `Idf` object and any other arguments as
   input, and returns a modified `Idf` object as output. This approach makes it
   easy for users to access to all the model-editing API that eplusr provides.

3. Create parametric models. The method `ParametricJob$apply_measure()` allows
   to apply a measure with specified parameter values to an `Idf` and create
   parametric models for later simulations. How the parameter values will be
   generated is all left to the users, which makes it possible to leverage all
   the statistical methods and libraries that R provides.

4. Run simulations and collect results. Calling `ParametricJob$run()` method
   will run all parametric simulations in parallel and put each simulation
   outputs in a separate folder. After the simulations complete, all results
   from different models can be retrieved in one step using all the data
   collecting APIs, including `ParametricJob$report_data()`,
   `ParametricJob$tabular_data()`, and `ParametricJob$read_table()`. As
   described above, the tables returned will always be presented in a
   tidy-format, which makes it easy to apply any further analyses in the R
   ecosystem.

One good example of extending the parametric framework in eplusr is the
epluspar [@Jia2020a] R package, which provides new classes for conducting
parametric analysis on EnergyPlus models, including sensitivity analysis using
Morris method [@Morris1991] and Bayesian calibration using the method proposed
by @Chong2017. All the new classes introduced in epluspar package are based on
the `ParametricJob` class. The main differences lie in applying specific
statistical methods for sampling parameter values when calling
`ParametricJob$apply_measure()`.

## Reproducible building energy performance simulation environment \note[HJ]{Further clarification needed}

The goal of reproducible research is to tie specific instructions to data
analysis and experimental data so that scholarship can be recreated, better
understood and verified [@Blischak2020]. With the increase in computational
tools comes the advantage of reproducibility at an extent not previously
possible.

Recently, Docker has risen to prominence for "containerisation", the method of taking
software artefacts and building them in such a way that use becomes
standardized and portable across operating systems, and has shown a promising
scalability opportunity [@Nust2020].

The R community has developed numerous packages to facilitate reproducible
research, covering a wide variety of topics, including  literate programming,
pipeline toolkits, package reproducibility, project workflows, code/data
formatting tools, format convertors, and object caching [@Blischak2020].

The Rocker Project was launched in 2014 as a collaboration to provide
high-quality Docker images containing the R environment, and has seen both
considerable uptake in the R community and substantial development and
evolution [@Boettiger2017]. The eplusr-Docker image [@Jia2020d] is built upon
the `rocker/verse` image. It contains a portable and reusable BES computation
environment powered by the RStudio server R programming environment,
EnergyPlus, eplusr package, together with all necessary packages for R-based
data-driven analytics using the tidyverse package and literate programming
environment using RMarkdown format and related packages.

A successful reproducible project consists of two major components [@Peng2015]:

* the raw data from the experiment are available
* and that the statistical code and documentation to reproduce that analysis
  are also available

In a time where BES and decision-makings based on BES are growing in
complexity, the need for reproducibility is growing significantly. From a BES
perspective, the raw data would be the raw building energy models or raw data
to build the models are available.

Recently, there have been significant efforts to develop software
infrastructure to reproducibly perform and communicate data analyses, including
RMarkdown [@Grolemund2018], Jupyter notebook [@Kluyver2016], just to name a few.

The RMarkdown format has been a
widely adopted authoring framework for data science. It can be used to both
save and execute code, and generate high-quality reports that can be shared
with an audience. RMarkdown can be easily adapted to any R-centric workflow and
is a good solution to facilitate reproducibility of BEM reporting, since both
the computing code and narratives are in the same document, and results are
automatically generated from the source code. RMarkdown supports dozens of
static and dynamic/interactive output formats.

### Tidyverse for data-driven analytics

Tidyverse is a collection of R packages that share a high-level design
philosophy and low-level grammar and data structures, so that learning one
package makes it easier to learn the next. The tidyverse encompasses the
repeated tasks at the heart of every data science project: data import,
tidying, manipulation, visualisation, and programming [@Wickham2019a].

### RMarkdown for literate programming

The R package knitr [@Xie2015] is a general-purpose literate programming
engine, with lightweight API's designed to give users full control of the
output without heavy coding work. Together with rmarkdown [@Allaire2020]
package and tinytex [@Xie2019] package, knitr is able to compile RMarkdown
documents to PDF, and also ensures a LaTeX document is compiled for the correct
number of times to resolve all cross-references.

# Examples and applications

## Data exploration

```{r read-idf, eval = FALSE}
# load package
library(eplusr)

# use example model and weather file
path_model <- file.path(eplus_config(9.1)$dir, "ExampleFiles/RefBldgMediumOfficeNew2004_Chicago.idf")
path_weather <- file.path(eplus_config(9.1)$dir, "WeatherData/USA_IL_Chicago-OHare.Intl.AP.725300_TMY3.epw")

# read model
idf <- read_idf(path_model)

idf$Output_Variable
```

## Data interface

```{r}
# run simulation
job <- model$run(path_epw)

# get metadata of all reported variable
str(job$report_data_dict())

# get the transformer input electric power at 11 a.m for the first day of
# RunPeriod named `SUMMERDAY`, tag this simulation as case `example`, and
# return all possible output columns.
power <- job$report_data("transformer 1", "transformer input electric power", case = "example",
  all = TRUE, simulation_days = 1, environment_name = "summerday", hour = 11, minute = 0)

# get all report variable with Celsius degree unit
str(job$report_data(job$report_data_dict()[units == "C"]))
```

## Parametric simulation of various infiltration rate on energy consumption

```{r}
# create a ParametricJob object
param <- param_job(path_idf, path_epw)

# create a measure for modifying infiltration rate
set_infil_rate <- function (idf, infil_rate) {
  # validate input value
  # this is optional, as validations will be made when setting values
  stopifnot(is.numeric(infil_rate), infil_rate >= 0)
  if (!idf$is_valid_class("ZoneInfiltration:DesignFlowRate"))
    stop("Input model does not have any object in class `ZoneInfiltration:DesignFlowRate`")
  # get all object IDS
  ids <- idf$object_id("ZoneInfiltration:DesignFlowRate", simplify = TRUE)
  # make a list of new values to set
  new_val <- list(design_flow_rate_calculation_method = "AirChanges/Hour", air_changes_per_hour = infil_rate)
  # create proper format for all objects in that class
  val <- rep(list(new_val), length(ids))
  names(val) <- paste0("..", ids)
  idf$set(val)
  idf
}

# create parametric models
param$apply_measure(set_infil_rate, seq(0, 4, by = 1), .names = NULL)

# run in parallel and collect results
param$run(wait = TRUE)

# see the variations of total energy
tab <- param$tabular_data(
    table_name = "Site and Source Energy",
    column_name = "Total Energy",
    row_name = "Total Site Energy"
)
total_eng <- tab[, list(case, `Total Energy (GJ)` = as.numeric(value))]
```

## Multi-optimization using Genetic Algorithm

```{r}
# specify file paths
path_db_win <- here::here("win.idf")
path_db_wall <- here::here("wall.idf")

# create a GA optimization job
ga <- GAOptimJob$new(path_idf, NULL)

# create a function to modify external walls and windows
use_tech <- function (idf, extwall_index = 1L, win_name = "ori") {
    # use ext wall
    if (extwall_index == 0) return(idf)

    const <- read_idf(path_db_wall)$Construction[[sprintf("concrete wall %i", extwall_index)]]
    if (is.null(const)) stop("Invalid wall index")

    dt <- data.table::rbindlist(c(list(const$to_table()), lapply(const$ref_to_object(), function (x) x$to_table())))
    idf$load(dt)

    id_extwall <- idf$to_table(class = "BuildingSurface:Detailed", wide = TRUE)[
        `Outside Boundary Condition` == "Outdoors" & `Surface Type` == "Wall", id]

    idf$set(.(id_extwall) := list(`Construction Name` = const$name()))

    # use window
    if (name == "ori") return(idf)

    win <- read_idf(path_db_win)$Construction[[win_name]]
    dt <- data.table::rbindlist(c(list(win$to_table()), lapply(win$ref_to_object(), function (x) x$to_table())))
    if (is.null(win)) stop("Invalid window name")

    idf$load(dt)
    idf$set("FenestrationSurface:Detailed" := list(`Construction Name` = name))

    idf
}

# specify how to modify the model
ga$apply_measure(
    measure = use_tech,
    extwall_index = integer_space(0:7),
    win_name = choice_space(c(
        "ori",
        "single clear glazing",
        "double clear 16",
        "double clear 25",
        "double lowe 16",
        "double lowe 25",
        "double lowe 25 both coated",
        "triple glazing system",
        "quadruple glazing system"
    ))
)

# define a function to coil desing cooling load
cal_coilloads <- function (idf) {
    job <- idf$last_job()
    stopifnot(!is.null(job))

    job$read_table("ComponentSizes")[comp_type == "Coil:Cooling:Water" & description == "Design Size Design Coil Load", sum(value)]
}

# specify the objective function
ga$objective(cal_coilloads, .dir = "min")

# specify the selector
ga$selector(survival = ecr::selGreedy)

# specify the maximum generation
ga$terminator(max_gen = 20L)

# run optimization
ga$run(mu = 10, dir = here::here("results"))

# read optimization results
read.csv(here::here("Results-2020-05-15.csv"))
```

## Model calibration using Bayesian theory

The `BayesCalibJob` class in the epluspar package implements Bayesian
calibration algorithms proposed by @Chong2017. The basic workflows of
conducting a Bayesian calibration using epluspar package are:

1. Set input and output variables using `$input()` and `$output()`
   respectively.
1. Add parameters to calibrate using `$param()` or `$apply_measure()`.
1. Check parameter sampled values and generated parametric models using
   `$samples()` and `$models()` respectively.
1. Run EnergyPlus simulations in parallel using `$eplus_run()`.
1. Gather simulated data of input and output parameters using `$data_sim()`.
1. Specify field measured data of input and output parameters using
   `$data_field()`.
1.  Specify field measured data of input and output parameters using `$data_field()`.
1. Specify input data for Stan for Bayesian calibration using `$data_bc()`.
1. Run Bayesian calibration and get predictions via Stan using `$stan_run()`.

Listing \@ref(code:bc)

```{r bc, code.cap = "Code for Bayesian calibration"}
# create a `BayesCalibJob` object:
bc <- bayes_job(path_idf, path_epw)

# setting Input and Output Variables
bc$input("CoolSys1 Chiller 1", paste("chiller evaporator", c("inlet temperature", "outlet temperature", "mass flow rate")), "hourly")
bc$output("CoolSys1 Chiller 1", "chiller electric power", "hourly")

# add Parameters to calibrate
bc$param(
    `CoolSys1 Chiller 1` = list(reference_cop = c(4, 6), reference_capacity = c(2.5e6, 3.0e6)),
    .names = c("cop1", "cap1"), .num_sim = 5
)

# get sample values and parametric models
bc$samples()

# run simulations and gather data
bc$eplus_run(dir = tempdir(), run_period = list("example", 7, 1, 7, 3), echo = false)
bc$data_sim("6 hour")

# specify measured data
# NOTE: for demonstration, we use the seed model to generate fake measured output data.
seed <- bc$seed()$clone()
seed$RunPeriod <- NULL
seed$add(RunPeriod = list("test", 7, 1, 7, 3))
seed$SimulationControl$set(
    `Run Simulation for Sizing Periods` = "No",
    `Run Simulation for Weather File Run Periods` = "Yes"
)
seed$save(tempfile(fileext = ".idf"))
job <- seed$run(bc$weather(), echo = FALSE)
fan_power <- epluspar:::report_dt_aggregate(job$report_data(name = bc$output()$variable_name, all = TRUE), "6 hour")
fan_power <- eplusr:::report_dt_to_wide(fan_power)
fan_power <- fan_power[, -"Date/Time"][
    , lapply(.SD, function (x) x + rnorm(length(x), sd = 0.05 * sd(x)))][
    , lapply(.SD, function (x) {x[x < 0] <- 0; x})
]

# set field data
bc$data_field(fan_power)

# specify input data for stan
str(bc$data_bc())

# run Bayesian calibration using Stan
res <- bc$stan_run(iter = 300, chains = 3)

# check calibrated results
rstan::stan_trace(res$fit)
rstan::stan_hist(res$fit)

# check predicted values
str(res$y_pred)
```

# Conclusion

Building energy simulation (BES) has been widely adopted for investigation of
environmental and energy performance for different design and retrofit
alternatives. The absence of seamless integration of BES and data-centric
analysis raises problems in both the productivity and also the credibility of
BES study. This paper proposed a novel holistic framework called 'eplusr' to
bridge the gap between the building energy simulation and data science domains.

Eplusr differs from existing frameworks by its data-centric design philosophy.
It provides a Tidy data format for BES that matches the semantics of the
simulation results. The result extraction interface provides the possibilities
to query any of the mentioned information which makes it easy and quite
straightforward to get simulation results of any specified time, units and
variables in a consistent manner. The Tidy-formatted results can be easily fed
to various data-centric analytics using existing tools in R.

This framework provides a prototype class to perform parametric simulations. It takes full
advantages of eplusr model editing API and Tidy data extraction functionalities.
It is capable of defining various analyses using
any algorithms available in R. The flexibility and extensibility of the
parametric simulation prototype in this framework is demonstrated by its easy
adoption to perform multi-objective optimization and Bayesian calibration.

The need for reproducibility in BEM is growing significantly together with the
ongoing trend of the increasing complexity of BEM project. The eplusr Docker image
provides a possible solution for this by providing a portable and reusable BES
computation environment containing EnergyPlus, eplusr R package, together with
all necessary packages for data-driven analytics and literate programming
environment.

# Acknowledgements

This research was funded by the Republic of Singapore's National Research
Foundation through a grant to the Berkeley Education Alliance for Research in
Singapore (BEARS) for the Singapore-Berkeley Building Efficiency and
Sustainability in the Tropics (SinBerBEST) Program. BEARS has been established
by the University of California, Berkeley as a center for intellectual
excellence in research and education in Singapore.

# References {#references .unnumbered}
